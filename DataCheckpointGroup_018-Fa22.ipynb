{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ant Man\n",
    "- Hulk\n",
    "- Iron Man\n",
    "- Thor\n",
    "- Wasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your research question here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: {sys.executable}: command not found\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nba_api in /home/jjconte/.local/lib/python3.9/site-packages (1.1.13)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from nba_api) (2.26.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from nba_api) (1.21.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->nba_api) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->nba_api) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->nba_api) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->nba_api) (2.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandera\n",
      "  Downloading pandera-0.13.4-py3-none-any.whl (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from pandera) (21.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from pandera) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.9/site-packages (from pandera) (1.21.1)\n",
      "Collecting typing-inspect>=0.6.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->pandera) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.2.0->pandera) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.2.0->pandera) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pandera) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.6.0->pandera) (3.10.0.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, mypy-extensions, wrapt, typing-inspect, pydantic, pandera\n",
      "Successfully installed mypy-extensions-0.4.3 pandera-0.13.4 pydantic-1.10.2 typing-extensions-4.4.0 typing-inspect-0.8.0 wrapt-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.9/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.9/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "# # upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "\n",
    "#install external libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install nba_api # nba stats\n",
    "!{sys.executable} -m pip install pandera # data validation \n",
    "# !{sys.executable} -m pip install pandera[io] # data validation + yaml output --error\n",
    "\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# team info\n",
    "from nba_api.stats.static import teams\n",
    "\n",
    "# team stats\n",
    "from nba_api.stats.endpoints import teamyearbyyearstats as teamyears\n",
    "\n",
    "import pandera as pa\n",
    "from pandera import Check, Column, DataFrameSchema\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TEAM_ID   TEAM_CITY   TEAM_NAME     YEAR  GP  WINS  LOSSES  WIN_PCT  \\\n",
      "0   1610612737  Tri-Cities  Blackhawks  1949-50  64    29      35    0.453   \n",
      "1   1610612737  Tri-Cities  Blackhawks  1950-51  68    25      43    0.368   \n",
      "2   1610612737   Milwaukee       Hawks  1951-52  66    17      49    0.258   \n",
      "3   1610612737   Milwaukee       Hawks  1952-53  71    27      44    0.380   \n",
      "4   1610612737   Milwaukee       Hawks  1953-54  72    21      51    0.292   \n",
      "..         ...         ...         ...      ...  ..   ...     ...      ...   \n",
      "28  1610612766   Charlotte     Hornets  2018-19  82    39      43    0.476   \n",
      "29  1610612766   Charlotte     Hornets  2019-20  65    23      42    0.354   \n",
      "30  1610612766   Charlotte     Hornets  2020-21  72    33      39    0.458   \n",
      "31  1610612766   Charlotte     Hornets  2021-22  82    43      39    0.524   \n",
      "32  1610612766   Charlotte     Hornets  2022-23   8     3       5    0.375   \n",
      "\n",
      "    CONF_RANK  DIV_RANK  ...  OREB  DREB   REB   AST    PF  STL   TOV  BLK  \\\n",
      "0           0         3  ...     0     0     0  1330  2057    0     0    0   \n",
      "1           0         5  ...     0     0     0  1476  2092    0     0    0   \n",
      "2           0         5  ...     0     0     0  1229  1848    0     0    0   \n",
      "3           0         5  ...     0     0     0  1427  2120    0     0    0   \n",
      "4           0         4  ...     0     0     0  1298  1771    0     0    0   \n",
      "..        ...       ...  ...   ...   ...   ...   ...   ...  ...   ...  ...   \n",
      "28          9         2  ...   814  2778  3592  1905  1550  591  1001  405   \n",
      "29         10         4  ...   715  2066  2781  1549  1223  428   949  268   \n",
      "30         10         4  ...   762  2389  3151  1933  1298  565  1069  344   \n",
      "31         10         3  ...   888  2767  3655  2302  1629  707  1087  402   \n",
      "32         12         4  ...    92   277   369   221   161   60   121   49   \n",
      "\n",
      "     PTS  PTS_RANK  \n",
      "0   5313        10  \n",
      "1   5730         3  \n",
      "2   4833        10  \n",
      "3   5389         9  \n",
      "4   5038         9  \n",
      "..   ...       ...  \n",
      "28  9081        19  \n",
      "29  6687        30  \n",
      "30  7881        23  \n",
      "31  9457         4  \n",
      "32   907        16  \n",
      "\n",
      "[1597 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION\n",
    "team_ids = [t['id'] for t in teams.get_teams()]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# slow but effective\n",
    "for i in team_ids:\n",
    "    df = pd.concat([df, teamyears.TeamYearByYearStats(i).get_data_frames()[0]])\n",
    "    time.sleep(0.2) # any lower and it'll time out\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Team Stats](https://github.com/swar/nba_api/blob/master/docs/nba_api/stats/endpoints/teamyearbyyearstats.md)\n",
    "\n",
    "**gameplan for anish and baraa**\n",
    "\n",
    "- data overview\n",
    "    - look over the [nba_api website](https://github.com/swar/nba_api) to fully understand what we're working with\n",
    "    - read in the data from jackson\n",
    "- cleaning\n",
    "    - get team stats (n = 30)\n",
    "        - how? team id/name\n",
    "    - potential columns needed: \n",
    "    - steps \n",
    "        1. **Validity**\n",
    "            1. data type and range (numbers and dates) constraints \n",
    "            1. foreign key (FK) constraint (FK column can't have a value that doesn't exist in the referenced primary key )\n",
    "            1. cross-field validation (e.g. ```FG3M``` $\\leq$ ```FG3A```)\n",
    "            1. **to make this go faster, the [pandera](https://pypi.org/project/pandera/) library can validate our data** \n",
    "                1. [Article 1](https://towardsdatascience.com/data-validation-for-pandas-b24613959364)\n",
    "                1. [Article 2](https://towardsdatascience.com/validate-your-pandas-dataframe-with-pandera-2995910e564)\n",
    "        1. **Inspect**\n",
    "            1. summary statistics\n",
    "            1. missing values\n",
    "            1. unique values \n",
    "            1. distribution\n",
    "            1. relationship (```FG3A``` &harr; ```FG3M```)\n",
    "            1. visualize for outliers\n",
    "            1. ```great_expectations``` library\n",
    "        1. **Clean**\n",
    "            1. irrelevant data (remove unneeded columns)\n",
    "            1. drop duplicates\n",
    "            1. type conversion (e.g. dates &rarr; ```df[\"col\"] = to_datetime(df[\"col\"])```)\n",
    "            1. remove white space\n",
    "            1. fix typos (**ewwww**)\n",
    "            1. check for missing data that is labeled (e.g. ```0```, ```NA```, ```INF```)\n",
    "            1. standardize (e.g. Strings &rarr; all upper/lower case, Season ```2022-2023``` &rarr; ```2022```)\n",
    "            1. scale?? \n",
    "            1. missing values!!! \n",
    "                - 3 options: Drop if rare, impute (mean if data is not skewed, median if data is skewed), leave it as is/recode to a missing indicator)\n",
    "                - *missing data doesn't equal unknown* per se (NBA website is relaible, so we shouldn't deal with this hopefully)\n",
    "            1. create new columns &rarr; data transformation? \n",
    "        1. **Verify**\n",
    "            1. Redo constraints check (see validation) w/ ```pandera``` library **assuming if we replaced the missing data**\n",
    "        6. **Export Data** \n",
    "            1. One DF as a ```csv``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok = True)\n",
    "df.to_csv(\"data/nba_stats_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "1. data type and range (numbers and dates) constraints\n",
    "1. foreign key (FK) constraint (FK column can't have a value that doesn't exist in the referenced primary key )\n",
    "1. cross-field validation (e.g. ```FG3M``` $\\leq$ ```FG3A```)\n",
    "1. **to make this go faster, the [pandera](https://pandera.readthedocs.io/en/stable/index.html) library can validate our data** \n",
    "    1. [Article 1](https://towardsdatascience.com/data-validation-for-pandas-b24613959364)\n",
    "    1. [Article 2](https://towardsdatascience.com/validate-your-pandas-dataframe-with-pandera-2995910e564)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"TEAM_ID\", \"TEAM_CITY\", \"TEAM_NAME\", \"YEAR\", \"GP\", \"WINS\", \"LOSSES\", \"WIN_PCT\",\n",
    "         \"PO_WINS\", \"PO_LOSSES\", \"NBA_FINALS_APPEARANCE\",\n",
    "   \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\", \"FT_PCT\", \"PTS\", \"PTS_RANK\"]]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_ids = [t[\"id\"] for t in teams.get_teams()]\n",
    "team_nickname = [t[\"nickname\"] for t in teams.get_teams()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"TEAM_ID\": Column(int, Check.isin(team_ids)),\n",
    "        \"TEAM_CITY\": Column(str),\n",
    "        \"TEAM_NAME\": Column(str, Check.isin(team_nickname)),\n",
    "        \"YEAR\": Column(str),\n",
    "        \"GP\": Column(int, Check.greater_than(0), Check.less_than(83)),\n",
    "        \"WINS\": Column(int, Check.greater_than(-1), Check.less_than(83)),\n",
    "        \"LOSSES\": Column(int, Check.greater_than(-1), Check.less_than(83)),\n",
    "        \"WIN_PCT\": Column(float, Check.greater_than(-1), Check.less_than(1.0001)),\n",
    "        \"PO_WINS\": Column(int, Check.greater_than(-1), Check.less_than(29)),\n",
    "        \"PO_LOSSES\": Column(int, Check.greater_than(-1), Check.less_than(29)),\n",
    "        \"NBA_FINALS_APPEARANCE\": Column(str),\n",
    "        \"FGM\": Column(int, Check.greater_than(-1)),\n",
    "        \"FGA\": Column(int, Check.greater_than(-1)),\n",
    "        \"FG_PCT\": Column(float, Check.greater_than(-1), lambda x: x == (x[\"FGM\"] / x[\"FGA\"])),\n",
    "        \"FG3M\": Column(int, Check.greater_than(-1)),\n",
    "        \"FG3A\": Column(int, Check.greater_than(-1)),\n",
    "        \"FG3_PCT\": Column(float, Check.greater_than(-1), lambda x: x == (x[\"FG3M\"] / x[\"FG3A\"])),\n",
    "        \"FTM\": Column(int, Check.greater_than(-1)),\n",
    "        \"FTA\": Column(int, Check.greater_than(-1)),\n",
    "        \"FT_PCT\": Column(float, Check.greater_than(-1), lambda x: x == (x[\"FTM\"] / x[\"FTA\"])),\n",
    "        \"PTS\": Column(int),\n",
    "        \"PTS_RANK\": Column(int, Check.greater_than(0), Check.less_than(31)),   \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_errors = pd.DataFrame()\n",
    "df_errors = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    schema.validate(df, lazy=True)\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    print(\"Schema errors and failure cases:\")\n",
    "    #print(err.failure_cases)\n",
    "    schema_errors = err.failure_cases\n",
    "    print(\"\\nDataFrame object that failed validation:\")\n",
    "    #print(err.data)\n",
    "    df_errors = err.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_errors[schema_errors[\"failure_case\"] == \"Bullets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.makedirs(\"tests\", exist_ok = True)\n",
    "\n",
    "yaml_schema = schema.to_yaml()\n",
    "\n",
    "f = Path(\"tests/nba_stats_schema.yml\")\n",
    "f.touch()\n",
    "f.write_text(yaml_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection\n",
    "\n",
    "1. summary statistics\n",
    "1. missing values \n",
    "1. unique values \n",
    "1. distribution\n",
    "1. relationship (```FG3A``` &harr; ```FG3M```)\n",
    "1. visualize for outliers\n",
    "1. ```great_expectations``` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "1. irrelevant data (remove unneeded columns)\n",
    "1. drop duplicates\n",
    "1. type conversion (e.g. dates &rarr; ```df[\"col\"] = to_datetime(df[\"col\"])```)\n",
    "1. remove white space\n",
    "1. fix typos (**ewwww**)\n",
    "1. check for missing data that is labeled (e.g. ```0```, ```NA```, ```INF```)\n",
    "1. standardize (e.g. Strings &rarr; all upper/lower case, Season ```2022-2023``` &rarr; ```2022```)\n",
    "1. scale?? \n",
    "1. missing values!!! \n",
    "    - 3 options: Drop if rare, impute (mean if data is not skewed, median if data is skewed), leave it as is/recode to a missing indicator)\n",
    "    - *missing data doesn't equal unknown* per se (NBA website is relaible, so we shouldn't deal with this hopefully)\n",
    "1. create new columns &rarr; data transformation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "1. Redo constraints check (see validation) w/ ```pandera``` library **assuming if we replaced the missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data\n",
    "\n",
    "1. One DF as a ```csv``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/nba_stats_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
